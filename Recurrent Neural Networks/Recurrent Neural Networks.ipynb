{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Traditional artificial neural networks obtain \"long-term memory\" by learning the adequate weights. The idea behind recurrent neural networks is to model \"short-term memory\". By connecting the hidden layer of the last observation to the hidden layer of the current observation, we allow the network to include knowledge of previous layers.\n",
    "\n",
    "A generic RNN, with input $u_t$ and state $x_t$ for time step $t$ is given by:\n",
    "\n",
    "$$x_t=F(x_{t-1}, u_t, \\theta)$$\n",
    "\n",
    "<img src='resources/rnn.png' width=800>\n",
    "\n",
    "A task displays long-term dependencies if prediction of the desired output at time $t$ depends on input presented at an earlier time $\\tau < t$. Bengio *et al*. (1994) proposed that a dynamical system that can learn to store relevant state information requires the following: \n",
    "\n",
    "1. Ability to store information for an arbitrary duration (information latching).\n",
    "2. Resistance to noise.\n",
    "3. Ability to train parameters in reasonable time.\n",
    "\n",
    "Traditional backpropagation in RNN is not sufficiently powerful to discover contingencies spanning long temporal intervals, resulting in parameters settling in sub-optimal solutions focused on short-time dependencies, but not long-term dependencies. This is mainly due to the vanishing and exploding gradient problems, in which the gradient either exponentially loses or gains magnitude as it travels back through the network.\n",
    "\n",
    "There are several solutions to this problem, including:\n",
    "\n",
    "* Long Short-Term Memory Networks\n",
    "* Echo-state Networks\n",
    "* Hessian-Free optimization with structural damping\n",
    "* Gradient clipping\n",
    "* Vanishing gradient regularization\n",
    "\n",
    "## Types of RNN\n",
    "\n",
    "* One-to-Many - image captioning\n",
    "* Many-to-One - sentiment analysis\n",
    "* Many-to-Many - neural machine translation, video captioning\n",
    "\n",
    "<img src='resources/types.png' width=600>\n",
    "\n",
    "## References\n",
    "\n",
    "* Bengio, Y., Simard, P., and Frasconi, P. (1994). [Learning Long-Term Dependencies with Gradient Descent is Difficult](http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf).\n",
    "* Pascanu, R., Mikolov, T., and Bengio, Y., (2013). [On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.pdf).\n",
    "* [Long Short-Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Understanding LSTM and its diagrams](https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714)\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Visualizing and Understanding Recurrent Networks](https://arxiv.org/pdf/1506.02078.pdf)\n",
    "* [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Networks\n",
    "\n",
    "LSTMs are a type of recurrent neural network module designed to avoid the vanishing and exploding gradients problem.\n",
    "\n",
    "<img src='resources/lstm.png'>\n",
    "\n",
    "<img src='resources/lstm-components.png'>\n",
    "\n",
    "## Cell State\n",
    "\n",
    "<img src='resources/lstm-cell-state.png'>\n",
    "\n",
    "The cell state is the core idea behind LSTMs. It is like a conveyor belt, running straight down the entire network, with only some linear interactions, making it easy for information to flow along it unchanged. \n",
    "\n",
    "The LSTM has the ability to remove or add information to the cell state, regulated by gates. Gates are a way to optionally let information through. They are composed out of a sigmoid activation function and a pointwise operation.\n",
    "\n",
    "## Forget Gate Layer\n",
    "\n",
    "The forget gate layer decides which information to throw away from the cell state. This decision is made by outputting a number between $0$ and $1$ for each number in the cell state $C_{t-1}$.\n",
    "\n",
    "For example, the network might want to remember the gender of the present subject, so that the correct pronouns can be used. When a new subject is seen, we want to forget the gender of the old subject.\n",
    "\n",
    "<img src='resources/lstm-forget-gate.png'>\n",
    "\n",
    "$$f_t=\\sigma(W_f(h_{t-1}\\oplus x_t) + b_f)$$\n",
    "\n",
    "## Input Gate Layer\n",
    "\n",
    "The next step is to decide what information to store in the cell state. First, a sigmoid layer decides which values we'll update. Next, a $\\tanh$ layer creates a vector of new candidate values, $\\tilde{C}_t$, that could be added to the cell state. Finally, these two are combined to create an update to the state.\n",
    "\n",
    "<img src='resources/lstm-input-gate.png'>\n",
    "\n",
    "$$ i_t = \\sigma(W_i(h_{t-1}\\oplus x_t) + b_i) $$\n",
    "\n",
    "$$ \\tilde{C}_t = \\tanh(W_C(h_{t-1}\\odot x_t) + b_C) $$\n",
    "\n",
    "Now, we update the cell state by forgetting the things we decided to forget, and then adding the new candidate values, scaled by how much we decided to update each state value.\n",
    "\n",
    "<img src='resources/lstm-cell-update.png'>\n",
    "\n",
    "$$ C_t = f_tC_{t-1} + i_t\\tilde{C}_t $$\n",
    "\n",
    "For example, this is the moment where the cell state forgets the previous subject's gender and remembers the current subject's gender.\n",
    "\n",
    "## Output Gate Layer\n",
    "\n",
    "Finally, we need to decide what we're going to output. This will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides which part of the cell state we're going to output. Then, we put the cell state through a $\\tanh$ (to push the values between $-1$ and $1$), and multiply these two together so that we only output the parts we decided to.\n",
    "\n",
    "<img src='resources/lstm-output-gate.png'>\n",
    "\n",
    "$$ o_t = \\sigma(W_O(h_{t-1}\\oplus x_t) + b_O) $$\n",
    "\n",
    "$$ h_t = o_t\\tanh(C_t) $$\n",
    "\n",
    "Here, the network might output information relevant to the next layers. For example: whether the subject is singular or plural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants on Long Short-Term Memory Networks\n",
    "\n",
    "## LSTM with peephole connections\n",
    "\n",
    "Allow gate layers to look at the cell state.\n",
    "\n",
    "<img src='resources/lstm-peepholes.png'>\n",
    "\n",
    "$$ f_t = \\sigma(W_f(C_{t-1}\\oplus h_{t-1}\\oplus x_t) + b_f) $$\n",
    "\n",
    "$$ i_t = \\sigma(W_i(C_{t-1}\\oplus h_{t-1}\\oplus x_t) + b_i) $$\n",
    "\n",
    "$$ o_t = \\sigma(W_o(C_{t}\\oplus h_{t-1}\\oplus x_t) + b_o) $$\n",
    "\n",
    "## LSTM with coupled forget and input gates\n",
    "\n",
    "Jointly decide which information to forget and add. Only forget when we're going to input something in its place, and only input new values to the state when we forget something older.\n",
    "\n",
    "<img src='resources/lstm-coupled.png'>\n",
    "\n",
    "$$ C_t = f_tC_{t-1} + (1-f_t)\\tilde{C}_t $$\n",
    "\n",
    "## Gated Recurrent Units\n",
    "\n",
    "GRUs combine the forget and input gates into a single \"update gate\". Also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models and has been growing increasingly popular.\n",
    "\n",
    "<img src='resources/gru.png'>\n",
    "\n",
    "$$ z_t = \\sigma(W_z(h_{t-1}\\oplus x_t)) $$\n",
    "\n",
    "$$ r_t = \\sigma(W_r(h_{t-1}\\oplus x_t)) $$\n",
    "\n",
    "$$ \\tilde{h}_t = \\tanh(W(r_th_{t-1}\\oplus x_t)) $$\n",
    "\n",
    "$$ h_t = (1-z_t)h_{t-1} + z_t\\tilde{h}_t $$\n",
    "\n",
    "## Other extensions\n",
    "\n",
    "* Depth-Gated RNNs https://arxiv.org/pdf/1508.03790v2.pdf\n",
    "* Clockwork RNNs https://arxiv.org/pdf/1402.3511v1.pdf\n",
    "* Attention mechanisms https://arxiv.org/pdf/1502.03044v2.pdf\n",
    "* Grid LSTMs https://arxiv.org/pdf/1507.01526v1.pdf\n",
    "* RNNs in generative models https://arxiv.org/pdf/1502.04623.pdf, https://arxiv.org/pdf/1506.02216v3.pdf, https://arxiv.org/pdf/1411.7610v3.pdf\n",
    "* [Greff, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) compare several popular variants, finding that they're all about the same. [Jozefowicz, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) tested thousands of RNN architectures, finding some that worked better than LSTMs on certain tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
